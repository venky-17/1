{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a4069a",
   "metadata": {},
   "source": [
    "# Program 6 >> Demonstrate Q learning algorithm with Suitable assumption for a problem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b54b503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-table:\n",
      "[[[ 5.30736345  6.98337296  3.5880942   0.        ]\n",
      "  [ 6.05534459  7.35091891  1.78085566  5.21931695]\n",
      "  [ 5.88073513  7.73780937  0.          0.        ]\n",
      "  [ 7.428297    8.1450625   3.14486283  6.7563636 ]\n",
      "  [ 7.15660378  0.          8.57375     0.        ]]\n",
      "\n",
      " [[ 6.25159877  0.          0.          0.        ]\n",
      "  [ 6.05534183  0.          0.          0.        ]\n",
      "  [ 4.69879506  0.          0.          0.        ]\n",
      "  [ 7.73664447  0.          0.          0.        ]\n",
      "  [ 6.51605     0.          9.025       5.32740455]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 7.27883162  0.          0.          0.        ]\n",
      "  [ 8.17492868  0.          9.5         5.32958793]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 4.38525245  0.          0.          0.        ]\n",
      "  [ 8.65577392  9.49159014 10.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_size = 5\n",
    "goal_state = (grid_size-1, grid_size-1)\n",
    "obstacles = [(2, 2), (3, 1)]\n",
    "\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "num_episodes = 1000\n",
    "\n",
    "q_table = np.zeros((grid_size, grid_size, 4))\n",
    "\n",
    "def take_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice([0, 1, 2, 3]) \n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "def update_q_value(state, action, reward, next_state):\n",
    "    old_q_value = q_table[state][action]\n",
    "    next_max_q = np.max(q_table[next_state])\n",
    "    new_q_value = old_q_value + learning_rate * (reward + discount_factor * next_max_q - old_q_value)\n",
    "    q_table[state][action] = new_q_value\n",
    "    \n",
    "def get_next_state(state, action):\n",
    "    row, col = state\n",
    "\n",
    "    next_row = row + (action == 2) - (action == 0)\n",
    "    next_col = col + (action == 1) - (action == 3)\n",
    "    if 0 <= next_row < grid_size and 0 <= next_col < grid_size:\n",
    "        if (next_row, next_col) not in obstacles:\n",
    "            return (next_row, next_col) \n",
    "    return state  \n",
    "\n",
    "def get_reward(state):\n",
    "    if state == goal_state:\n",
    "        return 10\n",
    "    elif state in obstacles:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_goal_state(state):\n",
    "    return state == goal_state\n",
    "\n",
    "def is_obstacle_state(state):\n",
    "    return state in obstacles\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)\n",
    "    done = False\n",
    "    epsilon = 1.0 / (episode + 1)\n",
    "\n",
    "    while not done:\n",
    "        action = take_action(state, epsilon)\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = get_reward(next_state)\n",
    "\n",
    "        update_q_value(state, action, reward, next_state)\n",
    "\n",
    "\n",
    "        state = next_state\n",
    "        done = is_goal_state(state) or is_obstacle_state(state)\n",
    "\n",
    "\n",
    "print(\"Final Q-table:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878def0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
